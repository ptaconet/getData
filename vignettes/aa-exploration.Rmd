---
title: "Automatic extraction of spatial-temporal environmental data within buffers around sample points"
author: "Paul Taconet"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Automatic extraction of spatial-temporal environmental data within buffers around sample points}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  
  
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r call_lib , include = FALSE}
library(dplyr)
library(sf)
library(mapview)
library(kableExtra)
```

## Objectives and presentation of the case-study

The objective of our work is to extract a set of time-space statistics of various climatic / environmental / landscape variables at provided dates and geographic locations. We showcase how we script this and optimize the overall processing performance with the R packages `getRemoteData`, `purrr` and `furrr`.

Technically speaking, we need to extract : within a given **buffer size** (e.g. 1000 m) around a given **point location** (e.g. lat -10.5, lon -5.2), enventually at a given **date** (e.g. 2017-05-12), a **summary** (e.g. mean) of a given **variable** (e.g. land surface temperature). More realistically, we are interested in extracting these data for : 

- several couples of {dates, locations} ;
- several sizes of buffer ;
- several climatic / environmental parameters (e.g temperature, precipitation, vegetation indices, etc.) ;
- several days prior to the dates of interest.

We illustrate the method in this vignette with a classical case study from the epidemiological world. The final objective is to model the presence and abundance of several species of malaria vector mosquitoes using various climatic, environmental and other covariates. For this we have counts of mosquitoes collected in 32 villages in a rural area of Côte d'Ivoire. The mosquitoes have been collected in each village at 4 different sampling points and for 8 nights using the Human Landing Catch (HLC) method. 
The data analysis workflow consists in : 

- importing, tidying and summarizing the various climatic and environmental data for the dates and region of interest ;
- create explicative / predictive models of the presence and abundance of each species.

Here we present how we automatize the first step (data import, tidy and summarize) using R. More specifically, the workflow includes : 

- the downloading and pre-processing of heterogeneous data ; 
- the calculatation of a summary of each covariate around each sampling point for various buffers sizes ; 
- for some covariates (e.g. temperature or precipitation), we want to reapeat those operations for a set of days prior to the date of HLC, to study how environmental conditions influence the presence / development of the mosquitoes ;

The locations and dates for our case study are provided and mapped below.

```{r, echo=F, eval=T}
head(read.csv(system.file("extdata/CIV_df_dates_loc.csv", package = "getRemoteData")),row.names = F,10)
# *id* is a unique identifier for each sampling {location; date}. 
```

```{r, echo=F}
points<-read.csv(system.file("extdata/CIV_df_dates_loc.csv", package = "getRemoteData")) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326)
```

```{r, echo=F, eval=T}
mapview(points)
```

The table below provides all the covariates that we want to extract over 3 different buffer sizes (0.5km, 1km, 2km), at each HLC location and date, and eventually for a 40 time-lag days prior to each HLC dates : 

```{r, echo=F, eval=T}
tab_env_cov<-read.csv("/home/ptaconet/r_react/environmental_covariates_dictionary.csv",stringsAsFactors = F,fileEncoding = "latin1")
tab_env_cov %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

Within the workflow we optimize the performance at each processing step : 

- `getRemoteData` enables to optimize the data download process by retrieving the data strictly over the region of interest (e.g. download a 1° square area of interest instead of a whole 10° square MODIS tile) ;
- `purrr` and `furrr` enable parallel data download and processing.

Most of the data sources implemented in `getRemoteData` are open, free, and available at global scale, making the workflow reusable at (almost) any place worldwide. Some sources (e.g. MODIS or GPM) require to sign-up towards the data provider. Details on the data sources implemented within the `getRemoteData` package can be found with : 

```{r getAvailableDataSources_call, eval = TRUE, echo=T}
dataSources<-getRemoteData::getAvailableDataSources()
```
```{r getAvailableDataSources_eval, eval = FALSE, echo=F}
#datatable(dataSources, rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T) )
```

## Worflow setup

### Setup input parameters 

We first provide the input parameters : 

- Path to the local folder where the data will be downloaded (NB : make sure the folder has writing permissions) ;
- Path to the file containing the credentials to your EarthData account (needed to retrieve MODIS and GPM data) ;
- Path to the csv file containing the locations and dates of interest (i.e. date and lat/lon for which you are interested in downloading the data and extract statistics) ;

```{r setup_param, eval=T, echo=T, results='hide'}
## Setup path to the working directory (i.e. where all the data to downloaded will be stored)
path_to_working_directory<-"/home/ptaconet/Documents/react/data_BF"  
## Setup the path to the file containing the EarthData credentials (for NASA datasets). See example at system.file("example-data/credentials_earthdata.txt", package = "getRemoteData")
path_to_earthdata_credentials<-file.path(path_to_working_directory,"credentials_earthdata.txt")
## Setup the path to the input dataset of dates, latitudes and longitudes of interest. The dataset in our case is stored in a geopackage database. For the reproducibility we also provide the path to the same dataset as a simple csv file
path_to_db<-"/home/ptaconet/Documents/react/react_db.gpkg"
query_input_dates_loc<-"SELECT idpointdecapture as id, date_capture as date, X as longitude, Y as latitude FROM hlc_dates_loc_times WHERE codepays='BF'"
# or : path_to_input_dates_loc <- system.file("extdata/CIV_df_dates_loc.csv", package = "getRemoteData")
# Setup buffer sizes in meters (i.e. buffers within which the statistics will be calculated)
buffer_sizes<-c(500,1000,2000)
## Specific to time series data : 
#Setup the time frames/lags for time-series data
lagTime_timeSeries<-40
hourStart_Dday<-"18"
hourEnd_Dday<-"08"
# GPM, TAMSAT and ERA-5 data are originally provided at approx. respectively 10km, 4km and 27km spatial resolutions. Resample them spatially ? If TRUE, resample with use bilinear interpolation
gpm_resample<-TRUE
tamsat_resample<-TRUE
era5_resample<-TRUE
# Spatial resolution (in meters) for output resampled data. Must be setup only if xxx_resample is set to TRUE
gpm_resample_output_res<-250
tamsat_resample_output_res<-250
era5_resample_output_res<-1000


## For some covariates we need own data. These data are stored in the geopackage database. Here we provide the names of the tables that we will use. 
## To see the structure of each of these data, go to section 'Workflow preparation' where the data are opened and their structure is provided
# For the indices related to village settlements and population : 
db_table_pop_by_village<-"villages_loc_pop"  # table containing the population for each village
db_table_loc_households_by_village<-"villages_households_loc_pop"  # table containing the location of each house in each village
db_table_builtup_rast<-"landcover_bf_builtup" # built up raster
db_table_pedology_rast<-"pedology_bf"  # pedology raster
# For the indices related to Land use / land cover : 
db_table_metadata_lulc<-"landcovers_metadata" # This table contains the information to use to compute land use / land cover landscape metrics : the various LU/LC rasters, the path to the rasters, etc.
layers_lulc<-c("landcover_bf_L2","landcover_bf_L3","landcover_bf_L4","landcover_bf_L5","ESACCI-LC-L4-LC10-Map-20m-P1Y-2016-v1.0","W020N20_ProbaV_LC100_epoch2015_global_v2.0.1") # Names of the LU/LC layers to use for the computation of landscape metrics

## For the calculation of the DEM and DEM-related indices : 
# Path to the GRASS application folder, for the setup of the rgrass7 package
path_to_grassApplications_folder<-"/usr/lib/grass74" #<Can be retrieved with grass74 --config path . More info on the use of rgrass7 at https://grasswiki.osgeo.org/wiki/R_statistics/rgrass7
# Setup the threshold for the accumulaton raster dataset (all values above this threshold are considered as the hydrographic network)
threshold_accumulation_raster<-1000  # CIV: 800 ; BF: 1000    ## TODO NEXT DEV : automatize the thresholding (Otsu or other)
## For the calculation of hydromorphic soils
hydromorphic_classes_pixels<-c(2,3,8,9,10) # pixels values whose classes are considered hydromorphic.   for CIV: c(11,14,5,2,13)  for BF: c(2,3,8,9,10)
```

### Load packages

```{r setup_packages, eval = T, echo=T, results='hide'}
require(raster)
require(sp)
require(sf)
require(spatstat)
require(maptools)
require(rgrass7)
require(rgdal)
require(gdalUtils)
require(rgeos)
require(tidyverse)
require(httr)
require(furrr)
require(reticulate)
require(RSQLite)
require(landscapemetrics)
if(!require(getRemoteData)){
 require(devtools)
 devtools::install_github("ptaconet/getRemoteData")
}
require(getRemoteData)

```

### Setup functions

We setup some functions that will be used throughout the workflow to process multiple dates / times series at once and to fill the NAs in some of the datasets.

```{r setup_functions, eval=T}
# Reshaping a list to get a data.frame of data to download, used as input of the getRemoteData::downloadData function.
getDftoDl<-function(data_md){
  DftoDl<- data_md %>%
   flatten %>%
   do.call(rbind,.) %>%
   distinct %>%
   set_names("name","url","destfile")
  return(DftoDl)
}

# Get the names of the time-series rasters to use for each date
getRastersNames_timeSeries<-function(df_dates_loc,xxxData_md,dataCollection){
  rastsNames<-map(df_dates_loc$date_numeric,~pluck(xxxData_md,as.character(.))) %>%
    map(.,pluck(dataCollection)) %>%
    map(.,pluck("name")) %>%
    map(.,as.character)
  return(rastsNames)
}

# Get the path of local rasters
getRastersPaths<-function(xxxData_md,dataCollection){
  path<-modify(xxxData_md,dataCollection) %>%
    map(~list_modify(.,"url" = NULL)) %>%
    map(data.frame) %>%
    reduce(bind_rows) %>%
    dplyr::select(name,destfile) %>%
    unique
  return(path)
}

# Time series data: Extract a summary of a variable over 1 buffer
extractVar_ts_singleBuff<-function(rasts,names_rasts_to_use,spPoints,buffer_size,fun_summarize){
  res<-rasts[names_rasts_to_use] %>%  # filter only the rasters of the time series
    map_dfr(~raster::extract(.,spTransform(spPoints,proj4string(.)),buffer=buffer_size,fun=eval(parse(text=fun_summarize)), na.rm=TRUE, small=TRUE)) %>% # for each raster, calculate the stats
    #set_names(origin_date-as.numeric(names(.))) %>% ## to name by the number of days separating the date of interest from the the day of the data
    set_names(seq(0,ncol(.)-1,1)) %>%   ## to name by the lag index 
    mutate(id=as.character(spPoints$id)) %>%
    mutate(buffer=buffer_size) %>%
    gather(time_lag,val,-c(id,buffer)) %>%
    mutate(time_lag=as.numeric(time_lag))
  return(res) # to put in wide format : res <- res %>% unite(var,var,time_lag)) %>% spread(key=var,value=val)
}


# Time series data: Extract a summary of a variable over multiple buffers
extractVar_ts<-function(buffer_sizes,df_dates_loc,names_rasts_to_use,rasters,fun_summarize){
  res<-buffer_sizes %>% # for each buffer, calculate stats
    set_names %>%
    future_map_dfr(~pmap_dfr(list(names_rasts_to_use,df_dates_loc$sp_points,.,fun_summarize),
                             ~extractVar_ts_singleBuff(rasters,..1,..2,..3,..4)))  %>%
    #mutate(var=var_name) %>%
    dplyr::select(id,buffer,time_lag,val)
  # to put in wide format : lst_min <- lst_min %>% unite(var,var,time_lag) %>% spread(key=var,value=val)
  return(res)
}

# Static (non time-series) data: Extract a summary of a variable over 1 buffer
extractVar_rast_singleBuff<-function(rasts,spPoints,buffer_size,fun_summarize){
  res<-raster::extract(rasts,spTransform(spPoints,proj4string(rasts)),buffer=buffer_size,fun=eval(parse(text=fun_summarize)), na.rm=TRUE, small=TRUE,df=TRUE) %>% 
    mutate(id=as.character(spPoints$id)) %>%
    mutate(buffer=buffer_size) %>%
    select(-ID) %>%
    gather(var,val,-c(id,buffer))
  return(res)
}

# Static (non time-series) data: Extract a summary of line (vector) variable over 1 buffer
extractVar_line_singleBuff<-function(network_sf,sfPoints,buffer_size){
  buffer<-st_buffer(sfPoints,dist=buffer_size) # Create the buffer around each point
  sfPoints <- sfPoints %>%
      rename(geometry_pt=geometry) # rename point geometry column, else an error is sent back on the pipe flow
  res<-st_join(buffer,network_sf, join = st_intersects,left = TRUE) %>% # Get the lines inside each buffer
      left_join(as.data.frame(network_sf)) %>%  # get the line geometry 
      rename(geometry_line=geom) %>% # rename line geometry column to be more clear
      left_join(as.data.frame(sfPoints),by="id")  %>%  # get the point geometry
      mutate(dist_line_to_pt=st_distance(geometry_pt,geometry_line,by_element = T)) %>% # get the distance between each point and line
      st_drop_geometry() %>%
      dplyr::select(id,dist_line_to_pt,accumulation) %>%
      group_by(id) %>%
        summarise(mean_dist_to_stream=as.numeric(mean(dist_line_to_pt)),min_dist_to_stream=as.numeric(min(dist_line_to_pt)),length_stream=as.numeric(sum(dist_line_to_pt)),mean_acc_by_dist=as.numeric(mean(accumulation*dist_line_to_pt))) %>% # compute stats related to hydrographic network
    mutate(mean_dist_to_stream=na_if(mean_dist_to_stream,0)) %>%
   mutate(min_dist_to_stream=na_if(min_dist_to_stream,0))

  res <- res %>%
    mutate(buffer=buffer_size) %>%
    gather(var,val,-c(id,buffer)) 
  return(res)
}


# Spatially resample a product (using bilinear interpolation)
resampleProd<-function(rast,resample_output_res){
  resample_output_res<-getRemoteData::convertMetersToDegrees(resample_output_res,latitude_4326=mean(c(extent(rast)[3],extent(rast)[4])))
  r<-rast
  res(r)<-resample_output_res
  rast<-raster::resample(rast,r,method='bilinear')
}

# Impute NAs and setup quality flag
# qval=1 is for values that where retrieved using the LST 1km/1day products (best quality)
# qval=2 is for values that where retrieved using the temperature of the upper(s) buffer(s) for the same day / time lag
# qval=3 is for values that where retrieved using the mean temperature of the previous and the following day / time lag
# qval=4 is for values that where retrieved using the LST 1km/8day temperature for the week of interest
# qval=5 is for values that where retrieved using the mean temperature of the previous and the following LST 1km/8day temperature
# qval=0 is for NAs remaining after these imputings

# qval=1
fillNA_l1<-function(df,qvalue){
df <- df %>%
  mutate(qval=if_else(!is.na(val),qvalue,0))
return(df)
}

# qval=2
fillNA_l2<-function(df,qvalue){
df <- df %>%
  arrange(id,time_lag,buffer) %>%
  group_by(id,time_lag) %>%
  fill(val, .direction = "up") %>%
  mutate(qval=if_else((qval==0 & !is.na(val)),qvalue,qval)) %>%
  as.data.frame()
return(df)
}

# qval=3
fillNA_l3<-function(df,qvalue){
df <- df %>% 
  arrange(id,buffer,time_lag) %>% 
  mutate(val_ahead=lead(val)) %>% 
  mutate(val_lag=lag(val)) %>% 
  mutate(val_mean=map2_dbl(.x=val_ahead,.y=val_lag,~mean(c(.x,.y),na.rm=T))) %>% 
  mutate(val_mean=replace(val_mean, which(is.nan(val_mean)), NA)) %>% 
  mutate(val=if_else(!is.na(val),val,val_mean)) %>% 
  mutate(qval=if_else((qval==0 & !is.na(val)),qvalue,qval))  %>% 
  dplyr::select(-c(val_ahead,val_lag,val_mean))
return(df)
}

# qval=4
fillNA_l4<-function(df,df_lst8day,qvalue){
lst8day_as_1day <- df_lst8day %>%
  slice(rep(1:n(), each = 7)) %>%
  group_by(id,buffer) %>%
  mutate(time_lag=seq(0,n()-1,1))

df<-left_join(df,lst8day_as_1day,by=c("id","buffer","time_lag")) %>%
  mutate(val.x=if_else(!is.na(val.x),val.x,val.y)) %>%
  dplyr::select(-c(val.y,var.y,qval.y)) %>%
  rename(val=val.x,var=var.x,qval=qval.x) %>%
  mutate(qval=if_else((qval==0 & !is.na(val)),qvalue,qval))
return(df)
}

```

### Workflow preparation

Setup working directory

```{r setup_wd , eval=T, echo=T}
setwd(path_to_working_directory)
```

Read the credentials to EarthData for further download of MODIS and GPM data.

```{r prepare_earthdata , eval=T, echo=T, results='hide'}
earthdata_credentials<-readLines(path_to_earthdata_credentials)
username_EarthData<-strsplit(earthdata_credentials,"=")[[1]][2]
password_EarthData<-strsplit(earthdata_credentials,"=")[[2]][2]
httr::set_config(authenticate(user=username_EarthData, password=password_EarthData, type = "basic"))
```

Open the dataset that contains the dates and locations of interest.

```{r prepare_datesloc, eval=T, echo=T, message = FALSE}
con <- dbConnect(RSQLite::SQLite(),path_to_db)
hlc_dates_loc<-dbGetQuery(con, query_input_dates_loc)
# or : hlc_dates_loc<-read.csv(path_to_input_dates_loc,stringsAsFactors = F)
head(hlc_dates_loc)
```

From this file we create our area of interest (i.e. bounding box of all our observations) as a `sf` polygon object.
Whenever possible, the data will be downloaded strictly over this bounding box, hence speeding-up the downloading time.

```{r prepare_roi, eval=T, echo=T}
roi<-sf::st_as_sf(hlc_dates_loc, coords = c("longitude", "latitude"), crs = 4326) %>% 
  sf::st_bbox()

mean_roi_latitude<-mean(c(roi$ymin,roi$ymax))
roi[1]<-roi[1]-0.05-getRemoteData::convertMetersToDegrees(max(buffer_sizes),mean_roi_latitude) #xmin
roi[2]<-roi[2]-0.05-getRemoteData::convertMetersToDegrees(max(buffer_sizes),mean_roi_latitude) #ymin
roi[3]<-roi[3]+0.05-getRemoteData::convertMetersToDegrees(max(buffer_sizes),mean_roi_latitude) #xmin
roi[4]<-roi[4]+0.05-getRemoteData::convertMetersToDegrees(max(buffer_sizes),mean_roi_latitude) #ymin

roi<-roi %>% 
  sf::st_as_sfc() %>% 
  sf::st_sf()

utm_zone<-getRemoteData::getUTMepsg(roi)
roi_utm<-st_transform(roi,utm_zone)

# Points
spPoints <- SpatialPointsDataFrame(coords=data.frame(hlc_dates_loc$longitude,hlc_dates_loc$latitude),data=hlc_dates_loc,proj4string=CRS("+init=epsg:4326"))
sfPoints <- st_as_sf(spPoints)
```

We group the dataset by date of interest. Output is a tibble whose observations (i.e. rows) are, for each date of interest, a `SpatialPointsDataFrame` object with all the locations of interest for that date.

```{r prepare2_datesloc, eval=T, echo=T}
df_dates_loc<-hlc_dates_loc %>% 
  mutate(date=as.Date(date)) %>%
  group_by(date) %>%
  arrange(date) %>%
  nest(latitude,longitude,id) %>%
  set_names(c("date_date","coords")) %>%
  mutate(sp_points=map(coords,~SpatialPointsDataFrame(coords=data.frame(.$longitude,.$latitude),data=.,proj4string=CRS("+init=epsg:4326")))) %>%
  dplyr::select(-coords) %>%
  mutate(date_numeric=as.integer(date_date))  %>%
  mutate(lagTime_timeSeries=lagTime_timeSeries)
df_dates_loc

datesDday<-as.Date(unique(hlc_dates_loc$date))
head(datesDday)

datesTimesSeries<-df_dates_loc$date_date %>%
  map(~seq(.,.-lagTime_timeSeries,-1)) %>%
  unlist %>%
  unique %>%
  sort %>%
  as.Date(origin="1970-01-01")
head(datesTimesSeries)
```

Prepare parallel computing with the `furrr` package 

```{r prepare_parralel, eval=T, echo=T}
plan(multiprocess)
options(future.globals.maxSize= 20000*1024^2) # 20 GB for the max size to be exported for the furrr future expression (https://stackoverflow.com/questions/40536067/how-to-adjust-future-global-maxsize-in-r)
```

Setup GRASS environment

```{r prepare_grass, eval=F, echo=T}
loc <- rgrass7::initGRASS(path_to_grassApplications_folder, home=path_to_working_directory, gisDbase="GRASS_TEMP", override=TRUE,mapset = "PERMANENT" )
execGRASS("g.proj",flags="c",parameters = list(proj4=sf::st_crs(roi_utm)$proj4string))
```

## 1/ Temperature, Vegetation, Evapotranspiration

We download all the MODIS collections of interest in one single shot. Within `getRemoteData` we use the standard [OpenDAP protocol](https://en.wikipedia.org/wiki/OPeNDAP) to access MODIS data. In few words, OpenDAP enables to subset the dataset at the download (hence making the downloaded dataset lighter).

The MODIS OpeNDAP server is located [here](https://opendap.cr.usgs.gov/opendap/hyrax/)

Then we extract the statistics of each variable (LST, VI, ET) separately.

Check out the function `getData_modis` for additional information on the MODIS products. 

### 1.1/ Import

MODIS data come with a specific CRS. We first need to transform the CRS of the ROI from EPSG 4326 to the specific MODIS CRS.

```{r modis_getTile, eval=F, echo=T}
modisCrs<-"+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181 +b=6371007.181 +units=m +no_defs"
modisTile<-getRemoteData::getMODIStileNames(roi)
roi_bbox<-sf::st_bbox(sf::st_transform(roi,modisCrs))
```

We retrieve a set of parameters that will be parsed in the `getData_modis` function. If they are not provided as input arguments of the function, these parameters are automatically calculated within the function. However, providing them makes the process faster, especially if we loop on the function.

```{r modis_getOpenDAPMetadata, eval=F, echo=T}
cat("Retrieving information to download MODIS data on the OpeNDAP servers...\n")
modisOpenDAP_md<-c("MOD11A1.006","MYD11A1.006","MOD11A2.006","MYD11A2.006","MOD13Q1.006","MYD13Q1.006","MOD16A2.006","MYD16A2.006") %>% 
  data.frame(stringsAsFactors = F) %>%
  set_names("source") %>%
  mutate(OpendapURL=paste0("https://opendap.cr.usgs.gov/opendap/hyrax","/",source,"/",modisTile,".ncml")) %>%
                  mutate(Opendap_timeVector=map(OpendapURL,~getRemoteData::getOpenDAPvector(.,"time"))) %>%
                  mutate(Opendap_xVector=map(OpendapURL,~getRemoteData::getOpenDAPvector(.,"XDim"))) %>%
                  mutate(Opendap_yVector=map(OpendapURL,~getRemoteData::getOpenDAPvector(.,"YDim"))) %>%
                  mutate(Opendap_minLon=map(Opendap_xVector,.f=~which.min(abs(.x-roi_bbox$xmin))-1)) %>%
                  mutate(Opendap_maxLon=map(Opendap_xVector,.f=~which.min(abs(.x-roi_bbox$xmax))-1)) %>%
                  mutate(Opendap_minLat=map(Opendap_yVector,.f=~which.min(abs(.x-roi_bbox$ymax))-1)) %>%
                  mutate(Opendap_maxLat=map(Opendap_yVector,.f=~which.min(abs(.x-roi_bbox$ymin))-1)) %>%
                  mutate(roiSpatialIndexBound=pmap(list(Opendap_minLat,Opendap_maxLat,Opendap_minLon,Opendap_maxLon),.f=~c(..1,..2,..3,..4))) %>%
                  mutate(destFolder=file.path(path_to_working_directory,source)) %>%
                  mutate(dimensionsToRetrieve=case_when(source %in% c("MOD11A1.006","MYD11A1.006","MOD11A2.006","MYD11A2.006") ~ list(c("LST_Day_1km","LST_Night_1km")),
                                                        source %in% c("MOD13Q1.006","MYD13Q1.006") ~ list(c("_250m_16_days_NDVI","_250m_16_days_EVI")),
                                                        source %in% c("MOD16A2.006","MYD16A2.006") ~ list(c("ET_500m"))
                                                        ))

```

Then we retrieve the URLs of the datasets to download using the `getData_modis` function. 

```{r modis_getMetadata, eval=F, echo=T}
## Build list of datasets to DL for all MODIS collection and for all dates
modisData_md<-datesDday %>%
  set_names(as.numeric(.)) %>% # names will be numeric format of the dates (days since 1970-01-01)
  map(~pmap(list(.,pluck(modisOpenDAP_md,"source"),pluck(modisOpenDAP_md,"destFolder"),pluck(modisOpenDAP_md,"dimensionsToRetrieve"),pluck(modisOpenDAP_md,"Opendap_timeVector"),pluck(modisOpenDAP_md,"roiSpatialIndexBound")),
            ~getRemoteData::getData_modis(timeRange = c(..1-lagTime_timeSeries,..1),
                           collection=..2,
                           destFolder=..3,
                           modisTile=modisTile,
                           dimensions=unlist(..4),
                           OpenDAPtimeVector=unlist(..5),
                           roiSpatialIndexBound=unlist(..6),
                           download = F)) %>%
        set_names(pluck(modisOpenDAP_md,"source")))
```

We finally download the data, parrallelizing it with the `parallelDL` argument set to `TRUE`.

```{r modis_import, eval=F, echo=T}
DftoDl<-getDftoDl(modisData_md)
cat("Downloading MODIS data...\n")
Dl_res<-downloadData(DftoDl,username_EarthData,password_EarthData,parallelDL=TRUE)
# Check if the datasets were downloaded
head(file.exists(DftoDl$destfile))
```

### 1.2/ Tidy, transform

The data are downloaded. We now process each MODIS collection to extract the statistics on :

- the land surface temperature
- the vegetation indices
- the evapotranspiration

#### 1.2.1 / Temperature

We extract two variables within each buffer : the average **daily maximum** (lstMax) and **daily minimum** (LstMin) surface temperatures, using the MODIS LST 1km/1day products (MOD11A1.006 and MYD11A1.006 collections).

The data processing workflow for the extraction of the maximum (resp. minimum) daily temperatures is the following : 

- Create a multi-layer raster with Terra (MOD11A1) and Aqua (MYD11A1) products, using the band *LST_Day_1km* (resp. *LST_Night_1km*) ; 
- take the highest (resp. lowest) available value for each pixel ;
- extract the average temperature value within each buffer (with the `extract` function of the `raster` package);

```{r temperature_tidytransform, eval=F, echo=T}

extract_modis_lst<-function(MOD_collection, MYD_collection){
  # For each date of interest, get the related time-series rasters names (i.e. the whole set of 40 days rasters)
  rastsNames_LST<-getRastersNames_timeSeries(df_dates_loc,modisData_md,MOD_collection)
  # head(rastsNames_modLst)
  # Build the path to the Terra (MOD) and Aqua (MYD) products
  path_to_mod<-getRastersPaths(modisData_md,MOD_collection)
  path_to_myd<-getRastersPaths(modisData_md,MYD_collection)
  path_to_mod_myd<-merge(path_to_mod,path_to_myd,by="name",suffixes = c("_mod","_myd"))

  # Pre-process LstMin
  rasts_LstMin<-path_to_mod_myd %>%
    mutate(rast_mod=map(destfile_mod,~getRemoteData::prepareData_modis(.,"LST_Night_1km"))) %>%  # open Terra products as raster
    mutate(rast_myd=map(destfile_myd,~getRemoteData::prepareData_modis(.,"LST_Night_1km"))) %>%  # Open Aqua products as raster
    mutate(rast=map2(rast_mod,rast_myd,~min(.x,.y,na.rm = T))) %>%  # Take the minimum value available between Terra and Aqcua 
    mutate(rast=map(rast,~.-273.15)) %>%  # convert into celcius degrees
    pluck("rast") %>%
    set_names(path_to_mod_myd$name)

  # Pre-process LstMax
  rasts_LstMax<-path_to_mod_myd %>%
    mutate(rast_mod=map(destfile_mod,~getRemoteData::prepareData_modis(.,"LST_Day_1km"))) %>%
    mutate(rast_myd=map(destfile_myd,~getRemoteData::prepareData_modis(.,"LST_Day_1km"))) %>%
    mutate(rast=map2(rast_mod,rast_myd,~max(.x,.y,na.rm = T))) %>%
    mutate(rast=map(rast,~.-273.15)) %>%
    pluck("rast") %>%
    set_names(path_to_mod_myd$name)
   
  # Extract the variables 
  cat("Extracting LstMin...\n")
  lstMin<-extractVar_ts(buffer_sizes,df_dates_loc,rastsNames_LST,rasts_LstMin,"mean") # to put in wide format : lstMin <- lstMin %>% unite(var,var,time_lag) %>% spread(key=var,value=val)
  lstMin$var<-"lstMin"
  cat("Extracting LstMax...\n")
  lstMax<-extractVar_ts(buffer_sizes,df_dates_loc,rastsNames_LST,rasts_LstMax,"mean")
  lstMax$var<-"lstMax"
  #head(lstMin)
  
  return(list(lstMin,lstMax))
}

cat("extracting MODIS LST 1 day...\n")
lst1day<-extract_modis_lst("MOD11A1.006","MYD11A1.006")
lst1day[[1]]$var<-"TND"
lst1day[[2]]$var<-"TMD"
cat("extracting MODIS LST 8 days...\n")
lst8day<-extract_modis_lst("MOD11A2.006","MYD11A2.006")
lst8day[[1]]$var<-"TNW"
lst8day[[2]]$var<-"TMW"

## Impute NAs
lst8day<-lst8day %>%
  map(.,~fillNA_l1(.,1))

lst1day<-lst1day %>%
  map(.,~fillNA_l1(.,1)) %>%
  map(.,~fillNA_l2(.,2)) %>%
  map(.,~fillNA_l3(.,3)) %>%
  map2(.x=.,.y=lst8day,~fillNA_l4(.x,.y,4))

lst8day<-lst8day %>%
  map(.,~fillNA_l2(.,2)) %>%
  map(.,~fillNA_l3(.,3))

lst1day<-lst1day %>%
    map2(.x=.,.y=lst8day,~fillNA_l4(.x,.y,5))


TND<-lst1day[[1]]
TMD<-lst1day[[2]]
TNW<-lst8day[[1]]
TMW<-lst8day[[2]]

# Export
write.csv(TND,file.path(path_to_working_directory,"envCov_TND.csv"),row.names = F)
write.csv(TMD,file.path(path_to_working_directory,"envCov_TMD.csv"),row.names = F)
write.csv(TNW,file.path(path_to_working_directory,"envCov_TNW.csv"),row.names = F)
write.csv(TMW,file.path(path_to_working_directory,"envCov_TMW.csv"),row.names = F)
```

#### 1.2.2 / Vegetation

We extract two variables within each buffer : the average **Normalized Difference Vegetation Indice ** (NDVI) and **Enhanced Vegetation Indice** (EVI), using the MODIS VI 250m/16days products (MOD13Q1.006 and MYD13Q1.006 collections).

The data processing workflow for the extraction of the NDVI (resp. EVI) variables is the following : 

- Extract the *_250m_16_days_NDVI* (resp. *_250m_16_days_EVI*) bands from Terra (MOD13Q1) and Aqua (MYD13Q1) products. Terra and Aqua products are phased, enabling to have a 8-days temporal resolution ; 
- extract the average values of NDVI (resp. EVI) indice within each buffer ;

Contrary to the LST, there are very few NAs in the VI products, as the temporal resolution is 16 days.

```{r vegetation_tidytransform, eval=F, echo=T}
# For each date of interest, get the related time-series rasters names (i.e. the whole set of 40 days rasters)
rastsNames_VI<-map2(getRastersNames_timeSeries(df_dates_loc,modisData_md,"MOD13Q1.006"),getRastersNames_timeSeries(df_dates_loc,modisData_md,"MYD13Q1.006"),c) %>%
  map(as.numeric) %>%
  map(~sort(.,decreasing=TRUE)) %>%
  map(as.character)
   
# Build the path to the Terra (MOD) and Aqua (MYD) products
  path_to_mod<-getRastersPaths(modisData_md,"MOD13Q1.006")
path_to_myd<-getRastersPaths(modisData_md,"MYD13Q1.006")
path_to_mod_myd<-rbind(path_to_mod,path_to_myd) %>% arrange(name)
   
# Pre-process NDVI. Bad quality pixels are directly set to NA. We keep all the other pixels.
rasts_ndvi<-path_to_mod_myd %>%
     mutate(rast=map(destfile,~getRemoteData::prepareData_modis(.,"_250m_16_days_NDVI"))) %>%
     pluck("rast") %>%
     set_names(path_to_mod_myd$name) 
 
# Pre-process EVI
rasts_evi<-path_to_mod_myd %>%
     mutate(rast=map(destfile,~getRemoteData::prepareData_modis(.,"_250m_16_days_EVI"))) %>%
     pluck("rast") %>%
     set_names(path_to_mod_myd$name) 
     
# Extract the variables 
cat("Extracting ndvi...\n")
VNI<-extractVar_ts(buffer_sizes,df_dates_loc,rastsNames_VI,rasts_ndvi,"mean")
VNI$var<-"VNI"
cat("Extracting evi...\n")
VEI<-extractVar_ts(buffer_sizes,df_dates_loc,rastsNames_VI,rasts_evi,"mean")
VEI$var<-"VEI" 
rm(rasts_ndvi,rasts_evi)

## Impute NAs
VNI<-VNI %>%
  fillNA_l1(.,1) %>%
  fillNA_l2(.,2) %>%
  fillNA_l3(.,3)

VEI<-VEI %>%
  fillNA_l1(.,1) %>%
  fillNA_l2(.,2) %>%
  fillNA_l3(.,3)

# Export
write.csv(VNI,file.path(path_to_working_directory,"envCov_VNI.csv"),row.names = F)
write.csv(VEI,file.path(path_to_working_directory,"envCov_VEI.csv"),row.names = F)
```

#### 1.2.2 / Evapotranspiration

We extract one variable within each buffer : the **Evapotranspiration** (ET), using the MODIS ET 500m/8days products (MOD16A2.006 and MYD16A2.006 collections).

The data processing workflow for the extraction of the ET variable is the following : 

- Create a multi-layer raster with Terra (MOD16A2) and Aqua (MYD16A2) products, using the band *ET_500m* ; 
- Extract the average temperature value within each buffer ;


```{r evapotransiration_tidytransform, eval=F, echo=T}
# For each date of interest, get the related time-series rasters names (i.e. the whole set of 40 days rasters)
rastsNames_Et<-getRastersNames_timeSeries(df_dates_loc,modisData_md,"MOD16A2.006")
   
# Build paths to data
path_to_mod<-getRastersPaths(modisData_md,"MOD16A2.006")
path_to_myd<-getRastersPaths(modisData_md,"MYD16A2.006")
path_to_mod_myd<-merge(path_to_mod,path_to_myd,by="name",suffixes = c("_mod","_myd"))
   
# Pre-process
rasts_Et<-path_to_mod_myd %>%
     mutate(rast_mod=map(destfile_mod,~getRemoteData::prepareData_modis(.,"ET_500m"))) %>%
     mutate(rast_myd=map(destfile_myd,~getRemoteData::prepareData_modis(.,"ET_500m"))) %>%
     mutate(rast_mod=map(rast_mod,~clamp(.x,upper=32760,useValues=FALSE))) %>% ## Set pixel values >= 32760 (quality pixel values) to NA 
     mutate(rast_myd=map(rast_myd,~clamp(.x,upper=32760,useValues=FALSE))) %>% ## Set pixel values >= 32760 (quality pixel values) to NA 
     mutate(rast=map2(rast_mod,rast_myd,~mean(.x,.y,na.rm = T))) %>%
     pluck("rast") %>%
     set_names(path_to_mod_myd$name)
     
# Extract
cat("Extracting et...\n")
EVT<-extractVar_ts(buffer_sizes,df_dates_loc,rastsNames_Et,rasts_Et,"mean")
EVT$var<-"EVT"
rm(rasts_Et)

## Impute NAs
EVT<-EVT %>%
  fillNA_l1(.,1) %>%
  fillNA_l2(.,2) %>%
  fillNA_l3(.,3)

# Export
write.csv(EVT,file.path(path_to_working_directory,"envCov_EVT.csv"),row.names = F)
```

## 2/ Precipitation (source : GPM)

As for the MODIS products with `getData_modis`, `getData_gpm` internally uses the OpeNDAP protocol to download the GPM data produced by the USGS.
The GPM OpeNDAP server is located [here](https://gpm1.gesdisc.eosdis.nasa.gov/opendap/GPM_L3/)

### 2.1/ Import 

For the GPM data we download two collections : the daily and the half-hourly precipitations. Daily precipitations will be useful to study the influence of precipitations for the development and abundance of the mosquitoes, while half-hourly precipitations will be used only over the night of catch.

```{r precipitationGpm_getOpenDAPMetadata, eval=F, echo=T}
roi_bbox<-st_bbox(roi)
cat("Retrieving information to download GPM data on the OpeNDAP servers...\n")
gpmOpenDAP_md<-c("GPM_3IMERGDF.06","GPM_3IMERGHH.06") %>% 
     data.frame(stringsAsFactors = F) %>%
     set_names("source") %>%
     mutate(OpendapURL="https://gpm1.gesdisc.eosdis.nasa.gov/opendap/GPM_L3/GPM_3IMERGHH.06/2016/001/3B-HHR.MS.MRG.3IMERG.20160101-S000000-E002959.0000.V06B.HDF5") %>%
     mutate(Opendap_xVector=map(OpendapURL,~getOpenDAPvector(.,"lon"))) %>%
     mutate(Opendap_yVector=map(OpendapURL,~getOpenDAPvector(.,"lat"))) %>%
     mutate(Opendap_minLon=map(Opendap_xVector,.f=~which.min(abs(.x-roi_bbox$xmin))-4)) %>%
     mutate(Opendap_maxLon=map(Opendap_xVector,.f=~which.min(abs(.x-roi_bbox$xmax))+4)) %>%
     mutate(Opendap_minLat=map(Opendap_yVector,.f=~which.min(abs(.x-roi_bbox$ymin))-4)) %>% ## careful, this line is not the same as for Modis. ymax has become ymin.
     mutate(Opendap_maxLat=map(Opendap_yVector,.f=~which.min(abs(.x-roi_bbox$ymax))+4)) %>%
     mutate(roiSpatialIndexBound=pmap(list(Opendap_minLat,Opendap_maxLat,Opendap_minLon,Opendap_maxLon),.f=~c(..1,..2,..3,..4))) %>%
     mutate(destFolder=file.path(path_to_working_directory,source)) %>%
    mutate(dimensionsToRetrieve=case_when(source %in% c("GPM_3IMERGDF.06") ~ list(c("precipitationCal")),
                                        source %in% c("GPM_3IMERGHH.06") ~ list(c("precipitationCal","precipitationQualityIndex"))
                                                        ))
``` 

Then we retrieve the URLs of the datasets to download using the `getData_gpm` function. 
  
```{r precipitationGpm_getMetadata, eval=F, echo=T}
## Build list of datasets to download for GPM Daily and for all dates
gpmOpenDAP_md_daily<-gpmOpenDAP_md %>% 
  filter(source=="GPM_3IMERGDF.06")

gpmData_md_daily<-datesDday %>%
     set_names(as.numeric(.)) %>% # names will be numeric format of the dates (days since 1970-01-01)
     map(~pmap(list(.,pluck(gpmOpenDAP_md_daily,"source"),pluck(gpmOpenDAP_md_daily,"destFolder"),pluck(gpmOpenDAP_md_daily,"dimensionsToRetrieve"),pluck(gpmOpenDAP_md_daily,"roiSpatialIndexBound")),
               ~getRemoteData::getData_gpm(timeRange = c(..1-lagTime_timeSeries,..1),
                              collection=..2,
                              destFolder=..3,
                              dimensions=unlist(..4),
                              roiSpatialIndexBound=unlist(..5))) %>%
           set_names(pluck(gpmOpenDAP_md_daily,"source")))
   
## Build list of datasets to download for GPM half-hourly and for all dates
gpmOpenDAP_md_hhourly<-gpmOpenDAP_md %>% 
  filter(source=="GPM_3IMERGHH.06")

gpmData_md_hhourly<-datesDday %>%
     set_names(as.numeric(.)) %>% # names will be numeric format of the dates (days since 1970-01-01)
     map(~pmap(list(.,pluck(gpmOpenDAP_md_hhourly,"source"),pluck(gpmOpenDAP_md_hhourly,"destFolder"),pluck(gpmOpenDAP_md_hhourly,"dimensionsToRetrieve"),pluck(gpmOpenDAP_md_hhourly,"roiSpatialIndexBound")),
               ~getRemoteData::getData_gpm(timeRange = c(paste0(as.Date(..1,origin="1970-01-01")," ",hourStart_Dday,":00:00"),paste0(as.Date(..1,origin="1970-01-01")+1," ",hourEnd_Dday,":00:00")),
                            collection=..2,
                            destFolder=..3,
                            dimensions=unlist(..4),
                            roiSpatialIndexBound=unlist(..5))) %>%
           set_names(pluck(gpmOpenDAP_md_hhourly,"source")))
``` 

We finally download the data, parrallelizing the processs with the `parallelDL` argument set to `TRUE`.

```{r precipitationGpm_import, eval=F, echo=T}
DftoDl<-rbind(getDftoDl(gpmData_md_daily),getDftoDl(gpmData_md_hhourly))
cat("Downloading GPM data...\n")
Dl_res<-downloadData(DftoDl,username_EarthData,password_EarthData,parallelDL=TRUE)
# Check if the datasets were downloaded
head(file.exists(DftoDl$destfile))
```
  

### 2.2/ Tidy, transform

We extract the following variables within the buffers for the time series : 

- *rain_gpmDay* : Mean of daily precipitations ;
- *rain_gpmHhour* : Proportions of half-hours with positive precipitation during the whole night of catch ;

#### 2.2.1/ Daily precipitations

```{r precipitationGpmDaily_tidytransform, eval=F, echo=T}
# For each date of interest, get the related time-series rasters names (i.e. the whole set of 40 days rasters)
rastsNames_gpmDay<-getRastersNames_timeSeries(df_dates_loc,gpmData_md_daily,"GPM_3IMERGDF.06")
   
# Build paths to data
path_to_gpmDay<-getRastersPaths(gpmData_md_daily,"GPM_3IMERGDF.06")
   
# Pre-process 
rasts_gpmDay<-path_to_gpmDay %>%
     mutate(rast=future_map(destfile,~getRemoteData::prepareData_gpm(.,"precipitationCal"))) %>% 
     pluck("rast") %>%
     set_names(path_to_gpmDay$name)

# resampling to 250 m
if(gpm_resample){
  rasts_gpmDay<-future_map(rasts_gpmDay,~resampleProd(.,gpm_resample_output_res)) 
}

# Extract
cat("Extracting daily precipitations (gpm)...\n")
RGD<-extractVar_ts(buffer_sizes,df_dates_loc,rastsNames_gpmDay,rasts_gpmDay,"mean")
RGD$var<-"RGD"
RGD$qval<-1
rm(rasts_gpmDay)

# Export
write.csv(RGD,file.path(path_to_working_directory,"envCov_RGD.csv"),row.names = F)
```   
  
#### 2.2.2/ Half-hourly precipitations

```{r precipitationGpmHhourly_tidytransform, eval=F, echo=T}
rastsNames_gpmHhour<-getRastersNames_timeSeries(df_dates_loc,gpmData_md_hhourly,"GPM_3IMERGHH.06")
   
# Build paths to data
path_to_gpmHhour<-getRastersPaths(gpmData_md_hhourly,"GPM_3IMERGHH.06")
   
# Pre-process
rasts_gpmHhour<-path_to_gpmHhour %>%
     mutate(rast=future_map(destfile,~getRemoteData::prepareData_gpm(.,"precipitationCal"))) %>% 
     mutate(rast_qval=future_map(destfile,~getRemoteData::prepareData_gpm(.,"precipitationQualityIndex"))) %>% 
     mutate(rast_qval=future_map(rast_qval,~clamp(.x,lower=0.4,useValues=FALSE))) %>% ## Set pixel values <= 0.4 (quality pixel values) to NA. See https://pmm.nasa.gov/sites/default/files/document_files/IMERGV06_QI.pdf for additional info (threshold of 0.4 is given in this doc)
     mutate(rast=future_map2(rast,rast_qval,~raster::mask(.x,.y))) %>% 
     pluck("rast") %>%
     set_names(path_to_gpmHhour$name)

# resampling to 250 m
if(gpm_resample){
  rasts_gpmHhour<-future_map(rasts_gpmHhour,~resampleProd(.,gpm_resample_output_res)) 
}

# Set pixel to 1 if there was rain (precip. >=0.05mm), else 0 
rasts_gpmHhour<-future_map(rasts_gpmHhour,~raster::reclassify(.,c(-Inf,0.05,0, 0.05,Inf,1)))

# Extract
cat("Extracting half hourly precipitations (gpm)...\n")
RGH<-extractVar_ts(buffer_sizes=10,df_dates_loc,rastsNames_gpmHhour,rasts_gpmHhour,"sum")  # With this we get if is has rain (1) or not (0) for each half hour of each night (ie the lag). This dataset will be useful to study the detailed rythms (cycles) of the mosquitoes. Note that ther is only 1 cell intersected (i.e. buffer_sizes=10) so the summurazing function is quite useless
# Now we extract the variable that we want: the proportion of half-hours where it has rained for the total duration of each night. There are quite few NAs: we remove them (ie we do not take them into account for the calculation of the proportions)
RGH<-RGH %>%
  filter(!is.na(val)) %>%
  group_by(id,buffer) %>% 
  summarise(val=round(sum(val)/n()*100))
RGH$var<-"RGH"
RGH$qval<-1
rm(rasts_gpmHhour)

# Export
write.csv(RGH,file.path(path_to_working_directory,"envCov_RGH.csv"),row.names = F)
```  

## 2/ Precipitation (source : TAMSAT)

TAMSAT is another source of precipitation data. https://www.tamsat.org.uk/about . We will use it to compare the model with GPM. 

### 3.1/ Import 

```{r precipitationTamsat_import, eval=F, echo=T}
tamsat_md<-data.frame(output_time_step=c("daily"), #,"monthly","monthly"),
                         output_product=c("rainfall_estimate"), #,"rainfall_estimate","anomaly"),
                         output_output=c("individual"), #,"individual","individual"),
                         stringsAsFactors = F) %>%
     mutate(source=paste(output_time_step,output_product,output_output,sep="_"))  %>%
     mutate(destFolder=file.path(path_to_working_directory,"TAMSAT",source)) 
   
tamsatData_md<-datesDday %>%
     set_names(as.numeric(.)) %>% # names will be numeric format of the dates (days since 1970-01-01)
     map(~pmap(list(.,pluck(tamsat_md,"destFolder"),pluck(tamsat_md,"output_time_step"),pluck(tamsat_md,"output_product"),pluck(tamsat_md,"output_output")),
               ~getRemoteData::getData_tamsat(timeRange = c(..1-lagTime_timeSeries,..1),
                            destFolder=..2,
                            output_time_step=..3,
                            output_product=..4,
                            output_output=..5)) %>%
                        set_names(pluck(tamsat_md,"source")))
   
## Download datasets
DftoDl<-getDftoDl(tamsatData_md)
cat("Downloading TAMSAT data...\n")
Dl_res<-downloadData(DftoDl,parallelDL=TRUE)
# Check if the datasets were downloaded
head(file.exists(DftoDl$destfile))
``` 

### 3.2/ Tidy, transform 

As for GPM, we extract the mean of daily precipitations (*rain_tamsatDay*) within the buffers for the time series.

```{r precipitationTamsat_tidytransform, eval=F, echo=T}
rastsNames_tamsat<-getRastersNames_timeSeries(df_dates_loc,tamsatData_md,"daily_rainfall_estimate_individual")
   
# Build paths to data
path_to_tamsat<-getRastersPaths(tamsatData_md,"daily_rainfall_estimate_individual")
   
# Pre-process
rasts_tamsat<-path_to_tamsat %>%
     mutate(rast=future_map(destfile,~getRemoteData::prepareData_tamsat(.,roi))) %>%
     pluck("rast") %>%
     set_names(path_to_tamsat$name)

# resampling to 250 m
if(tamsat_resample){
rasts_tamsat<-future_map(rasts_tamsat,~resampleProd(.,tamsat_resample_output_res)) 
}

# Extract
cat("Extracting daily precipitations (tamsat)...\n")
RTD<-extractVar_ts(buffer_sizes,df_dates_loc,rastsNames_tamsat,rasts_tamsat,"mean")
RTD$var<-"RTD"
RTD$qval<-1
rm(rasts_tamsat)

# Export
write.csv(RTD,file.path(path_to_working_directory,"envCov_RTD.csv"),row.names = F)
``` 

## 4/ Light/Settlements

### 4.1/ Import 

```{r lightSettlements_import, eval=F, echo=T}
viirsDnbData_md<-datesDday %>%
     set_names(as.numeric(.)) %>% # names will be numeric format of the dates (days since 1970-01-01)
     map(~map(.,
              ~getRemoteData::getData_viirsDnb(timeRange = .,
                           roi=roi,
                           dimensions=c("Monthly_AvgRadiance","Monthly_CloudFreeCoverage"),
                           destFolder=file.path(path_to_working_directory,"viirs_dnb"))) %>%
     set_names("viirs_dnb"))
   
   
## Download datasets
DftoDl<-getDftoDl(viirsDnbData_md)
cat("Downloading VIIRS DNB data...\n")
Dl_res<-downloadData(DftoDl,parallelDL=TRUE)
# Check if the datasets were downloaded
head(file.exists(DftoDl$destfile))
``` 

### 4.2/ Tidy, transform

```{r lightSettlements_tidytransform, eval=F, echo=T}
# Get the names of the rasters to use for each date
rastsNames_viirsDnb<-getRastersNames_timeSeries(df_dates_loc,viirsDnbData_md,"viirs_dnb") %>%
     map(~keep(.,str_detect(.,"Monthly_AvgRadiance")))

# Build paths to data
path_to_viirsDnb_AvgRadiance<-getRastersPaths(viirsDnbData_md,"viirs_dnb") %>%
     mutate(date=substr(name,nchar(name)-4,nchar(name))) %>%
     filter(str_detect(name,"AvgRadiance")) %>%
     set_names(c("name_AvgRadiance","destfile_AvgRadiance","date"))

path_to_viirsDnb_CloudFreeCoverage<-getRastersPaths(viirsDnbData_md,"viirs_dnb") %>%
     mutate(date=substr(name,nchar(name)-4,nchar(name))) %>%
     filter(str_detect(name,"CloudFreeCoverage")) %>%
     set_names(c("name_CloudFreeCoverage","destfile_CloudFreeCoverage","date"))
   
path_to_viirsDnb<-merge(path_to_viirsDnb_AvgRadiance,path_to_viirsDnb_CloudFreeCoverage,by="date")

# Pre-process
rasts_viirsDnb<-path_to_viirsDnb %>%
     mutate(rast_AvgRadiance=map(destfile_AvgRadiance,~raster(.))) %>% 
     mutate(rast_CloudFreeCoverage=map(destfile_CloudFreeCoverage,~raster(.))) %>% 
     mutate(rast_CloudFreeCoverage=map(rast_CloudFreeCoverage,~clamp(.x,lower=1,useValues=FALSE))) %>% # Quality control : if there are 0 cloud free obs, we set the pixel to NA
     mutate(rast=map2(rast_AvgRadiance,rast_CloudFreeCoverage,~mask(.x,.y))) %>%
     pluck("rast") %>%
     set_names(path_to_viirsDnb_AvgRadiance$name_AvgRadiance)

# Extract
cat("Extracting nighttime lights (VIIRS DNB...\n")
LNL<-extractVar_ts(buffer_sizes,df_dates_loc,rastsNames_viirsDnb,rasts_viirsDnb,"mean")
LNL$time_lag<-NULL
LNL$var<-"LNL"
LNL$qval<-1
rm(rasts_viirsDnb)

# Export
write.csv(LNL,file.path(path_to_working_directory,"envCov_LNL.csv"),row.names = F)
``` 


## 5/ Light/Moon

Ephemeris of the Moon

### 5.1/ Import 

```{r lightMoon_import, eval=F, echo=T}
moonData_md<-datesDday %>%
   set_names(as.numeric(.)) %>% # names will be numeric format of the dates (days since 1970-01-01)
   map(~map(.,
            ~getRemoteData::getData_imcce(timeRange = .,
                                roi=roi,
                                destFolder=file.path(path_to_working_directory,"moon_imcce"))) %>%
         set_names("moon_imcce"))
   
## Download datasets
df_DataToDL<-getDftoDl(moonData_md)
cat("Downloading IMCCE moon data...\n")
Dl_res<-downloadData(df_DataToDL,parallelDL=TRUE)
``` 

### 5.2/ Tidy, transform

```{r lightMoon_tidytransform, eval=F, echo=T}
# Get the names of the files to use for each date
DfNames_moon<-getRastersNames_timeSeries(df_dates_loc,moonData_md,"moon_imcce") %>%
     unlist() %>%
     cbind(names(moonData_md)) %>%
     data.frame(stringsAsFactors = F) %>%
     set_names(c("name","date"))

# Build paths to data
path_to_moon<-getRastersPaths(moonData_md,"moon_imcce")
   
# Pre-process
moon_magnitude_dfs<-path_to_moon %>% 
     mutate(df=map(destfile,~read.csv(.,skip=10))) %>% 
     mutate(V.Mag=map_dbl(df,"V.Mag"))
   
# Extract 
LMN<-hlc_dates_loc %>%
     dplyr::select(id,date) %>%
     mutate(date=as.character(as.numeric(as.Date(date)))) %>%
     left_join(DfNames_moon,by="date") %>%
     left_join(moon_magnitude_dfs,by="name") %>%
     dplyr::select(id,V.Mag) %>%
     set_names("id","val") %>%
     mutate(var="LMN") %>%
     mutate(buffer=NA) %>%
     mutate(qval=1) %>%
     dplyr::select(id,buffer,val,var,qval)

# Export
write.csv(LMN,file.path(path_to_working_directory,"envCov_LMN.csv"),row.names = F)
``` 

## 6/ Wind

### 6.1/ Import 

```{r wind_import, eval=F, echo=T}
era5_md<-data.frame(dimensionsToRetrieve=c("10m_u_component_of_wind","10m_v_component_of_wind"),
                       stringsAsFactors = F) %>%
  mutate(source=dimensionsToRetrieve)
     
era5Data_md<-datesDday %>%
     set_names(as.numeric(.)) %>% # names will be numeric format of the dates (days since 1970-01-01)
     map(~pmap(list(.,pluck(era5_md,"source"),pluck(era5_md,"dimensionsToRetrieve")),
               ~getRemoteData::getData_era5(timeRange = c(paste0(as.Date(..1,origin="1970-01-01")," ",hourStart_Dday,":00:00"),paste0(as.Date(..1,origin="1970-01-01")+1," ",hourEnd_Dday,":00:00")),
                            roi=roi,
                            destFolder=file.path(path_to_working_directory,..2),
                            dimensions=..3)) %>%
           set_names(era5_md$source))

# First create directories in the wd
directories<-era5_md$source %>% 
     file.path(path_to_working_directory,.) %>%
     as.list()  %>%
     lapply(dir.create,recursive = TRUE)
   
cat("Downloading ERA-5 wind data...\n")
# Then download
#import python CDS-API
cdsapi <- reticulate::import('cdsapi')
#for this step there must exist the file .cdsapirc in the root directory of the computer (e.g. "/home/ptaconet")
server = cdsapi$Client() #start the connection
# for now we have not found a better solution that making a "for" loop (it does not work as expected with purrr)
for (i in 1:length(era5Data_md)){
  for (j in 1:length(era5Data_md[[i]])){
     for (k in 1:length(era5Data_md[[i]][[j]]$destfile)){
       if (!file.exists(era5Data_md[[i]][[j]]$destfile[k])){
      server$retrieve("reanalysis-era5-single-levels",
                     era5Data_md[[i]][[j]]$url[k][[1]],
                     era5Data_md[[i]][[j]]$destfile[k])
        }
       }
     }
   }
```    

### 6.2/ Tidy, transform 

```{r wind_tidytransform, eval=F, echo=T}
# Get the names of the rasters to use for each date
rastsNames_era5<-getRastersNames_timeSeries(df_dates_loc,era5Data_md,"10m_u_component_of_wind")
   
# Build paths to data
path_to_u_wind<-getRastersPaths(era5Data_md,"10m_u_component_of_wind")
path_to_v_wind<-getRastersPaths(era5Data_md,"10m_v_component_of_wind")
path_to_u_v_wind<-merge(path_to_u_wind,path_to_v_wind,by="name",suffixes = c("_u","_v"))
   
# source : https://stackoverflow.com/questions/21484558/how-to-calculate-wind-direction-from-u-and-v-wind-components-in-r
rasts_wind<-path_to_u_v_wind %>%
       mutate(rast_u_wind=future_map(destfile_u,~getRemoteData::prepareData_era5(.)[[1]])) %>% 
       mutate(rast_v_wind=future_map(destfile_v,~getRemoteData::prepareData_era5(.)[[1]])) %>%
  mutate(rast_wind_speed=future_map2(rast_u_wind,rast_v_wind,~overlay(.x,.y,fun=function(.x,.y){return(sqrt(.x^2+.y^2))}))) %>%
  mutate(rast_wind_dir_cardinal=future_map2(rast_u_wind,rast_v_wind,~overlay(.x,.y,fun=function(.x,.y){return(atan2(.x,.y))}))) %>%
  mutate(rast_wind_dir_cardinal=future_map(rast_wind_dir_cardinal,~.*180/pi)) %>%
  mutate(rast_wind_dir_cardinal=future_map(rast_wind_dir_cardinal,~.+180)) %>%
  select(name,rast_wind_speed,rast_wind_dir_cardinal)
  

## resample wind speed but not wind direction 
if(era5_resample){
  rasts_wind <- rasts_wind %>%
    mutate(rast_wind_speed=future_map(rast_wind_speed,~resampleProd(.,era5_resample_output_res)))
}

rasts_wind_speed <- rasts_wind %>%
    pluck("rast_wind_speed") %>%
    set_names(path_to_u_wind$name)

rasts_wind_dir_cardinal <- rasts_wind %>%
    pluck("rast_wind_dir_cardinal") %>%
    set_names(path_to_u_wind$name)

rm(rasts_wind)

# Extract
cat("Extracting wind speed...\n")
WSP<-extractVar_ts(buffer_sizes=10,df_dates_loc,rastsNames_era5,rasts_wind_speed,"sum") # There is only 1 cell intersected (i.e. buffer_sizes=10) so the summurazing function is quite useless
WSP <- WSP %>%
  group_by(id,buffer) %>% 
  summarise(val=mean(val))
WSP$var<-"WSP"
WSP$qval<-1
rm(rasts_wind_speed)


cat("Extracting wind direction...\n")
# formula :  https://en.wikipedia.org/wiki/Mean_of_circular_quantities (see section "Example")
WDR<-extractVar_ts(buffer_sizes=10,df_dates_loc,rastsNames_era5,rasts_wind_dir_cardinal,"sum") # Same as above
WDR <- WDR %>%
  mutate(sin_angle=sin(val*(pi/180))) %>%
  mutate(cos_angle=cos(val*(pi/180))) %>%
  group_by(id,buffer) %>% 
  summarise(mean_sin=mean(sin_angle),mean_cos=mean(cos_angle)) %>% 
  mutate(val=atan(mean_sin/mean_cos)/(pi/180)) %>%
  mutate(val=case_when(mean_sin>0 & mean_cos>0 ~ val,
                       mean_cos<0 ~ val+180,                           
                       mean_sin<0 & mean_cos>0 ~ val+360)) %>%
  select(id,buffer,val)
WDR$var<-"WDR"
WDR$qval<-1
rm(rasts_wind_dir_cardinal)

# Export
write.csv(WSP,file.path(path_to_working_directory,"envCov_WSP.csv"),row.names = F)
write.csv(WDR,file.path(path_to_working_directory,"envCov_WDR.csv"),row.names = F)
```


## 7/ Topography

### 7.1/ Import

Setup the path to the output datasets that will be generated

```{r topography_setup, eval=F, echo=T}
destFolder_dem<-file.path(path_to_working_directory,"DEM_SRTM")
dem_output_path<-file.path(destFolder_dem,"DEM.tif")
dem_depressionless_output_path<-file.path(destFolder_dem,"DEM_depressionless.tif")
slope_output_path<-file.path(destFolder_dem,"slope.tif")
aspect_output_path<-file.path(destFolder_dem,"aspect.tif")
accumulation_output_path<-file.path(destFolder_dem,"accumulation.tif")
accumulation_threshold_output_path<-gsub(".tif","_treshold.tif",accumulation_output_path)
tci_output_path<-file.path(destFolder_dem,"tci.tif") 
twi_output_path<-file.path(destFolder_dem,"twi.tif") 
streams_network_path<-file.path(destFolder_dem,"streams_network.gpkg")
```

Download the SRTM tiles over our areas of interest

```{r topography_import, eval=F, echo=T}
DftoDl<-getRemoteData::getData_srtm(roi,destFolder=destFolder_dem)
cat("Downloading SRTM data...\n")
Dl_res<-getRemoteData::downloadData(DftoDl,username_EarthData,password_EarthData,parallelDL=TRUE)
# Check if the datasets were downloaded
head(file.exists(DftoDl$destfile))
```

### 7.2/ Tidy, transform

Prepare the DEM: unzip the files, merge the different tiles (if relevant), crop to the ROI and reproject to UTM projection

```{r topography_tidytransform1, eval=F, echo=T}
# Unzip and get path
strm_tiles<-map(DftoDl$destfile,~unzip(.,exdir=destFolder_dem))
# Create DEM in UTM projection and save to disk
DEM <- strm_tiles %>%
  map(~raster::raster(.)) %>%
  do.call(raster::merge, .) %>%
  raster::crop(roi) %>%
  raster::projectRaster(crs=sf::st_crs(roi_utm)$proj4string) %>%
  raster::writeRaster(dem_output_path,NAflag=0,overwrite=TRUE)
```

Create the topography datasets from the DEM with the `rgrass7` package : slope, aspect, accumulation, tci, twi

```{r topography_tidytransform2, eval=F, echo=T}
# Import DEM to GRASS and set region
rgrass7::execGRASS("r.external", flags="o", parameters=list(input=dem_output_path, output="tmprast",band=1))
rgrass7::execGRASS("g.region", parameters=list(raster="tmprast")) 

# Filters and generates a depressionless elevation map
rgrass7::execGRASS("r.fill.dir", flags="overwrite", parameters=list(input="tmprast", output="DEM",direction="dir"))
rgrass7::execGRASS("r.out.gdal", flags=c("t","m","overwrite"), parameters=list(input="DEM", output=dem_depressionless_output_path, format="GTiff",  createopt="TFW=YES,COMPRESS=LZW" ))
  
# Compute slope and aspect and save to disk
rgrass7::execGRASS("r.slope.aspect", flags="overwrite", parameters=list(elevation="DEM", slope="slope",aspect="aspect",format="percent", precision="FCELL",zscale=1,min_slope=0))
rgrass7::execGRASS("r.out.gdal", flags=c("t","m","overwrite"), parameters=list(input="slope", output=slope_output_path, format="GTiff",  createopt="TFW=YES,COMPRESS=LZW" ))
rgrass7::execGRASS("r.out.gdal", flags=c("t","m","overwrite"), parameters=list(input="aspect", output=aspect_output_path, format="GTiff",  createopt="TFW=YES,COMPRESS=LZW" ))

# Compute hydrograpy indices and save to disk
rgrass7::execGRASS("r.terraflow", flags="overwrite", parameters=list(elevation="DEM", accumulation="accumulation", tci="tci"))
rgrass7::execGRASS("r.out.gdal", flags=c("t","m","overwrite"), parameters=list(input="accumulation", output=accumulation_output_path, format="GTiff",  createopt="TFW=YES,COMPRESS=LZW" ))
rgrass7::execGRASS("r.out.gdal", flags=c("t","m","overwrite"), parameters=list(input="tci", output=tci_output_path, format="GTiff",  createopt="TFW=YES,COMPRESS=LZW" ))

# Compute TWI indice
rgrass7::execGRASS("r.topidx", flags="overwrite", parameters=list(input="DEM", output="twi"))
rgrass7::execGRASS("r.out.gdal", flags=c("t","m","overwrite"), parameters=list(input="twi", output=twi_output_path, format="GTiff",  createopt="TFW=YES,COMPRESS=LZW" ))
```

Create the stream network from the accumulation raster file. We threshold the accumulation : consider that all cells above a given accumulation (provided as input parameter `threshold_accumulation_raster`) are the hydrographic network and all the others are not. The `threshold_accumulation_raster` was determined manually by overlaying the accumulation raster file with High resolution satellite image.  
See example on the example provided here: <https://grass.osgeo.org/grass76/manuals/r.thin.html>

```{r topography_tidytransform3, eval=F, echo=T}
# Create accumulation threshold raster
# NB : Grass has a algorithm 'r.stream.extract' that can do the job (see https://grass.osgeo.org/grass76/manuals/addons/r.accumulate.htmlsœ)
acc_raster<-raster(accumulation_output_path) %>%
  raster::reclassify(c(-Inf,threshold_accumulation_raster,NA, threshold_accumulation_raster,Inf,1)) %>%
  raster::writeRaster(accumulation_threshold_output_path,datatype='INT2S',overwrite=TRUE)

# skeletonization (thinning extraction) and vectorization of stream network from flow accumulation map. See https://grass.osgeo.org/grass76/manuals/r.thin.html 
rgrass7::execGRASS("r.external", flags="overwrite", parameters=list(input=accumulation_threshold_output_path, output="acc_threshold",band=1))
rgrass7::execGRASS("g.region", parameters=list(raster="acc_threshold")) 
rgrass7::execGRASS("r.thin", flags="overwrite", parameters=list(input="acc_threshold",output="acc_thin"))
rgrass7::execGRASS("r.out.gdal", flags=c("t","m","overwrite"), parameters=list(input="acc_thin", output=file.path(path_to_dem_folder,"accumulation_thin.tif"), format="GTiff",  createopt="TFW=YES,COMPRESS=LZW" ))
rgrass7::execGRASS("r.to.vect", flags="overwrite", parameters=list(input="acc_thin", output="acc_thin_vect", type="line"))
# Next we need to execute the v.split algo. With v.split we split the stream network into pieces of 20 meters. This will be used then to measure the mean length between the HLC point and the streams within the buffer
# the step v.split does not give the appropriate results when executed in R, although it does not send back any error... It is weird because it works in QGIS (through GRASS plugin). For now we need to do it by hand in QGIS. To do it by hand execute: 
rgrass7::execGRASS("v.out.ogr", flags=c("m","overwrite"), parameters=list(input="acc_thin_vect", type="line", output=file.path(destFolder_dem,"acc_thin_vect.gpkg")))
# and than manually execute the algo v.split in the GRASS plugin of QGIS. Save the output file under streams_network_path
# To do it via R it should be : 
#rgrass7::execGRASS("v.split", flags=c("overwrite","verbose"), parameters=list(input="acc_thin_vect", output="acc_thin_vect_split", length=20, units="map"))
#rgrass7::execGRASS("v.out.ogr", flags=c("m","overwrite"), parameters=list(input="acc_thin_vect_split", type="line", output=streams_network_path))
```

Calculate the statistics for the elevation, slope, aspect, accumulation, tci, twi

```{r topography_tidytransform4, eval=F, echo=T}
dem_and_derivatives_rast <- brick(list(dem_depressionless_output_path,slope_output_path,aspect_output_path,accumulation_output_path,tci_output_path,twi_output_path))
names(dem_and_derivatives_rast)<-c("TEL","TSL","TAS","WAC","TCI","TWI")

TEL_TSL_TAS_WAC_TCI_TWI<-buffer_sizes %>% # for each buffer, calculate stats
    set_names %>%
    map_dfr(~extractVar_rast_singleBuff(dem_and_derivatives_rast,spPoints,.,"mean")) %>%
  mutate(val=if_else(var=="WAC",val*res(dem_and_derivatives_rast)[1]*res(dem_and_derivatives_rast)[2]/10000,val)) # convert accumulation from number of pixels to surface in ha

# Export
TEL_TSL_TAS_WAC_TCI_TWI<-TEL_TSL_TAS_WAC_TCI_TWI %>%
  mutate(qval=1)

write.csv(TEL_TSL_TAS_WAC_TCI_TWI,file.path(path_to_working_directory,"envCov_TEL_TSL_TAS_WAC_TCI_TWI.csv"),row.names = F)
```

Calculate the statistics linked to the stream network

```{r topography_tidytransform5, eval=F, echo=T}
# open stream network
streams_network_sf<-sf::read_sf(streams_network_path) %>%
  st_zm() %>%
  mutate(DN=seq(1,nrow(.),1))  %>%
  select(DN,geom)

# Get the accumulation value on each piece of the network. For this, take the centroid of each piece of network and calculate the accumulation on this piece.
accumulation<-raster(accumulation_output_path)
accumulation<-accumulation*res(accumulation)[1]*res(accumulation)[2]/10000 # convert accumulation from number of pixels to surface in ha

sp_cent <- rgeos::gCentroid(as(streams_network_sf, "Spatial"), byid=TRUE)
streams_network_sf<-raster::extract(accumulation,sp_cent,df=TRUE) %>%
  select(-ID) %>%
  mutate(DN=streams_network_sf$DN) %>%
  full_join(streams_network_sf,.)

# Convert the points to UTM projection
sfPoints_utm<-st_transform(sfPoints,utm_zone)  
  
# Calculate the stats for all buffers
WAD_WMD_WLS_WAL<-buffer_sizes %>%
  set_names() %>%
  future_map_dfr(~extractVar_line_singleBuff(streams_network_sf,sfPoints_utm,.))

# Export
WAD_WMD_WLS_WAL<-WAD_WMD_WLS_WAL %>%
  mutate(var=str_replace_all(var,c("mean_dist_to_stream"="WAD","min_dist_to_stream"="WMD","length_stream"="WLS","mean_acc_by_dist"="WAL"))) %>%
  mutate(qval=1)

write.csv(WAD_WMD_WLS_WAL,file.path(path_to_working_directory,"envCov_WAD_WMD_WLS_WAL.csv"),row.names = F)
```

## 8/ Settlements and population

### 8.1/ Import

For the covariates related to settlements and population in each village we use the surveys realized at the beginning of the REACT project.

First we open the other data that are stored on the database. These data will be used to compute some of the indices related to population and the spatial shape of the villages.

```{r settlementsPop_import, eval=F, echo=T}
con <- DBI::dbConnect(RSQLite::SQLite(),dbname = path_to_db)
sf_pop_by_village <- st_read(con,db_table_pop_by_village) %>%
  st_set_crs(4326) %>%
  st_intersection(roi) # keep only the points in our ROI
sf_households_by_village <- st_read(con,db_table_loc_households_by_village) %>%
  st_set_crs(4326) %>%
  st_intersection(roi) # keep only the points in our ROI

head(sf_pop_by_village)
head(sf_households_by_village)

```

### 8.2/ Tidy, transform

Extract information : population in each village, population density, built surface, distance from each catch point to the edge of the village

```{r settlementsPop_tidytransform, eval=F, echo=T}
# Get the villages convex hull (considered as the boundaries of the villages)
villages_convexhull <- sf_households_by_village %>%
  group_by(codevillage) %>%  # group by village
  summarize(geom_convHull=st_convex_hull(st_union(geom))) %>% #get convex hull polygon. We consider the convex hull as the edges of the village. The convex hull is the minimum polygon that encompasses all the locations of the households.
  st_transform(utm_zone)

## Get stats: population density
POP_POD <- villages_convexhull %>%
  left_join(as_tibble(sf_pop_by_village),by="codevillage") %>% # join the dataset of villages to get the population
  mutate(village_area=st_area(geom_convHull)*0.0001) %>% # get the area of the village in ha
  mutate(POD=population/village_area) %>% # get the population density of the village in pers/ha 
  rename(POP = population) %>% 
  dplyr::select(codevillage,POD,POP) %>% 
  st_drop_geometry() %>% 
  gather(var,val,-codevillage)

# Export
write.csv(POP_POD,file.path(path_to_working_directory,"envCov_POP_POD.csv"),row.names = F)

```

## 9/ Built-up

### 9.1/ Import

The buit-up dataset was computed using the SPOT6 image. The R script to generate built-up from the Spot 6 image is located [here](https://github.com/ptaconet/r_react/blob/master/database/builtup.R).

Here we import the dataset that was generated by that script and stored in the local database.

```{r builtup_import , eval=F, echo=T}
# we have not found the solution to read the rasters directly in the gpkg database. So we write them as tif : 
      path_to_builtup_rast<-file.path(path_to_working_directory,"Classification","bati_raster.tif")
#gdalUtils::gdal_translate(path_to_db,path_to_builtup_rast,ot="Int16",of="GTiff",oo=paste0("TABLE=",db_table_builtup_rast))
```

### 9.2/ Tidy, transform

surface built in each village + density of surface built around each catch point

```{r buitup_tidytransform, eval=F, echo=T}

## Get the distance from each catch point to the edge of the village
BDE <- sfPoints %>%
  st_transform(utm_zone) %>%
  mutate(codevillage=substr(id,2,4)) %>%  # create codevillage before joining with edges of the village
  left_join(as.data.frame(villages_convexhull),by="codevillage") %>%  # join convex hull geometry
  mutate(geom_convHull=st_cast(geom_convHull, "LINESTRING")) %>%  # convert convex hull geometry from polygon to linestring
  mutate(val=st_distance(geometry,geom_convHull,by_element=TRUE)) %>% # compute distance between catch point and edges of the village
  st_drop_geometry() %>%
  dplyr::select(id,val) %>% 
  mutate(var="VDE") %>% 
  mutate(buffer=NA) %>%
  mutate(qval=1)

## Get an indice of the clustering or ordering of the households in each village. For this we use the locations of the households with the function spatstat::clarkevans
BCH <- sf_households_by_village %>%
  st_drop_geometry()  %>% # put back as data.frame (non spatial)
  group_by(codevillage) %>%
  nest() %>%
  mutate(sp_points=map(data,~SpatialPoints(coords=data.frame(.$X,.$Y),proj4string=CRS("+init=epsg:4326")))) %>% 
  mutate(ppp=map(sp_points,~as(., "ppp"))) %>%   # uses the maptools package
  mutate(clark_index=map(ppp,~spatstat::clarkevans(.))) %>%
  mutate(clark_index=map_dbl(clark_index,~as.numeric(.[1]))) %>%
  dplyr::select(codevillage,clark_index) %>%
  set_names("codevillage","val") %>%
  mutate(var="VCH") 

# Read raster of built up areas
builtup_rast<-raster::raster(path_to_builtup_rast)

BVS <- villages_convexhull %>%
  mutate(val=map(geom_convHull,~raster::extract(builtup_rast,as(.,'Spatial'),fun=sum,na.rm=T))) %>% # get the built-up area by intersecting the built-up raster with the convex hull polygon of each village
    mutate(val=as.numeric(val)*res(builtup_rast)[1]*res(builtup_rast)[2]) %>% # convert to m2
  st_drop_geometry() %>%
  mutate(var="BVS") 

BCD<-raster::extract(builtup_rast,spTransform(spPoints,proj4string(builtup_rast)),buffer=100,fun=sum,na.rm=T,sp=TRUE) %>% # get the built-up area around each catch point by intersecting the built-up raster with a buffer of 100 m around each catch point
  as.data.frame() %>%
  select(id,bati_raster) %>%
  mutate(bati_raster=as.numeric(bati_raster)*res(builtup_rast)[1]*res(builtup_rast)[2]) %>% # convert to m2
  mutate(bati_raster=bati_raster/(pi*100^2)*100) %>% # convert to proportion
  set_names(c("id","var")) %>%
  mutate(val="BCD") %>%
  mutate(buffer=NA)  %>%
  mutate(qval=1)

# Export
write.csv(BDE,file.path(path_to_working_directory,"envCov_BDE.csv"),row.names = F)
write.csv(BCH,file.path(path_to_working_directory,"envCov_BCH.csv"),row.names = F)
write.csv(BVS,file.path(path_to_working_directory,"envCov_BVS.csv"),row.names = F)
write.csv(BCD,file.path(path_to_working_directory,"envCov_BCD.csv"),row.names = F)
```


## 10/ Pedology

### 10.1/ Import

The pedology dataset was computed using [TO FINISH]. The R script to generate the pedology dataset is located [here](https://github.com/ptaconet/r_react/blob/master/database/pedology.R).

Here we import the dataset that was generated by that script and stored in the local database.

```{r pedology_import , eval=F, echo=T}
# we have not found the solution to import the rasters directly from the gpkg database. So we read them in the db and write them as tif : 
path_to_pedology_rast<-file.path(path_to_working_directory,"pedology","pedology.tif")
#gdalUtils::gdal_translate(path_to_db,path_to_pedology_rast,ot="Int16",of="GTiff",oo=paste0("TABLE=",db_table_pedology_rast))
```

### 10.2/ Tidy, transform

```{r pedology_tidytransform, eval=F, echo=T}
pedology_rast<-raster::raster(path_to_pedology_rast)
names(pedology_rast)="PED"
# Set to 0 classes that are not hydromorphic and to 1 the classes that are hydromorphic
pedology_rast[!(pedology_rast %in% hydromorphic_classes_pixels)]<-0
pedology_rast[pedology_rast!=0]<-1

HYS<-buffer_sizes %>% # for each buffer, calculate stats
    set_names %>%
    map_dfr(~extractVar_rast_singleBuff(pedology_rast,spPoints,.,"sum"))

# convert surface from count of pixels to proportion (%) of hydromorphic soil in the buffer
HYS <- HYS %>%
  mutate(val=val*res(pedology_rast)[1]*res(pedology_rast)[2]) %>%
  mutate(val=val/(pi*buffer^2)*100) %>%
  mutate(qval=1)

# Export
write.csv(HYS,file.path(path_to_working_directory,"envCov_HYS.csv"),row.names = F)
```

## 11/ Land use / cover

### 11.1/ Import

We will compute the stats over several LU/LC datasets.

```{r lulc_import, eval=F, echo=T}
metadata_landcovers <- tbl(con,db_table_metadata_lulc) %>%
  as.data.frame() %>%
  distinct(layer_label,layer_path,layer_id) %>%
  filter(layer_label %in% layers_lulc)

lulc_rasts_list<-metadata_landcovers$layer_path %>%
  map(~raster(.)) %>%
  future_map(~crop(.,st_transform(roi,proj4string(.))))

spPoints$plot_id<-seq(1,nrow(spPoints),1)
```

### 11.2/ Tidy, transform

```{r lulc_tidytransform, eval=F, echo=T}

# Using future we get an error... no time to investigate why. So we do it linearly with a "for" loop.

for (i in 1:length(lulc_rasts_list)){
  # Calculate the landscape metrics for the raster i and for the 3 buffer sizes
  cat(paste0("Calculating landscape metrics for the ",length(buffer_sizes)," buffer sizes and LU/LC layer '",metadata_landcovers$layer_label[i], "' (layer n°",i," over ",nrow(metadata_landcovers),")...\n"))

  if(proj4string(lulc_rasts_list[[i]])=="+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"){
    buffer_sizes_th_rast=convertMetersToDegrees(buffer_sizes,mean(extent(lulc_rasts_list[[i]])[3],extent(lulc_rasts_list[[i]])[4]))
  } else {
    buffer_sizes_th_rast=buffer_sizes
  }
  
  res_th_rast <- buffer_sizes_th_rast %>% 
        set_names(buffer_sizes) %>% 
    map_dfr(~landscapemetrics::sample_lsm(lulc_rasts_list[[i]], 
                                   spTransform(spPoints,proj4string(lulc_rasts_list[[i]])), 
                                   what = c("lsm_c_ca", "lsm_l_ta"),
                                   shape = "circle",
                                   size = .,
                                   progress=T),
                               .id = "buffer")


  res_th_rast <- res_th_rast %>%
    select(-c(id,percentage_inside)) %>%
    rename(val=value,class_pixel=class) %>%
    left_join(as.data.frame(spPoints),by="plot_id") %>%
    mutate(layer_id=metadata_landcovers$layer_id[i]) %>%
    select(id,buffer,layer_id,class_pixel,level,metric,val)

  # Export, appending each time
  readr::write_csv(res_th_rast,file.path(path_to_working_directory,"envCov_landscapemetrics.csv"),append=T,col_names=if_else(file.exists(file.path(path_to_working_directory,"envCov_landscapemetrics.csv")),F,T))

}
```

## Close workflow

We finally remove the temporary files and disconnect from the DB

```{r setup_close, eval=F, echo=T}
# Remove grass temporary folder
system(paste0("rm -r ", fTile.path(path_to_working_directory,"GRASS_TEMP")))  
file.remove(file.path(path_to_working_directory,".grassrc7"))
# Disconnect from the local database
dbDisconnect(con)
```
